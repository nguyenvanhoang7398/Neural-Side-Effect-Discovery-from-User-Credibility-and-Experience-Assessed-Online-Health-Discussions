Minor revision (29/01/2020):

Reviewer reports:

Reviewer #2: Abstract
- The sentence ending in "the discussion level that addresses these shortcomings" is not clear. Consider rephrasing to "at the discussion level, thus, addressing the mentioned shortcomings"

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R2P1"

Page 4
- It is not possible to understand the meaning of the sentence starting in line 14 ("Our approachâ€¦"). Change "by jointly model" to "jointly modelling".

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R2P2"

- Line 19, the authors state that "Such weighted summation is mathematically shown in the Appendix", however, I can only find  three equations relating to a loss function in the appendix. It is unclear what the authors are referring to.

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R2P3"

Page 10
- Line 17, remove "of the Appendix."

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R2P4"

Page 12
- Line 15-16  is incomplete "We also observe a decrease in performance while optimizing the (?) without being aware of users."

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R2P5"

Consider adding uNEAT, NEAT, ADR, RF, and nDCG@2 to the abbreviation list.

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R2P6"

Table 9 does not require the Pre., Rec., and F1. information in the legend.

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R2P7"

Reviewer #3:
I have read the corrected version of the article and I have seen an improvement in different aspects: writing, wideness of experiments, and deepness of analysis. The revised version contains an advanced version with regard to what authors presented at the LOUHI workshop. The Section on Related Work is largely better. Providing the code in a github repository is an optimal help for reproducing their work.

In particular, authors have conducted an ablation analysis, and extended their work by adding a CNN architecture enhanced with their approach (attention-based modeling of user credibility + expertise integration). This is a nice contribution to the previous version; however, it seemed to me somewhat confusing because authors do not state explicitely whether the NEAT approach makes reference to the CNN or the LSTM, or to their method (which is implemented in both architectures and outperforms the baselines without their method)... 

> Min: Hoang, can you summarize what you did for key concerns (like the one here)?  Your replies are not as helpful 
> as they can be, because they force the reader to cross-reference the original submission.
> No need to address small typo and grammar changes in this requested summary.

In short, I propose authors be clearer regarding their contribution and what NEAT refers to (a combination of methods that can be implemented in both LSTM or CNN architectures?). I think it could be enough by adding to the abstract or the Introduction a similar sentence to the following (p. 9, l. 56ff): "We implement both CNN and LSTM-based text encoders to confirm the consistent improvement across different neural encoders.".

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R3P1" as part of our contributions

In my opinion, I think the paper is close to be published. Still, I have some minor concerns and comments that I explain below.

-Section 2, Section Drug Side Effect Discovery: Authors do not justify adequately how neural methods deal with lexical variation. It is true that some algorithms (e.g. fastText, Bojanowski et al. 2017*; FLAIR embeddings, Akbik et al. 2018**) model the morphology of words by leveraging subword or character information. But morphology is only one aspect of lexical variation; what about syntax (e.g. "acute myeloid leukemia" <-> "myeloid leukemia, acute") or semantics (e.g. synonyms: "cephalea" <-> "headache")? I think the main benefits of using neural networks are not stated adequately by posing the argument of lexical variation. Maybe authors should focus on the fact that neural networks seem adequate when a large amount of data is available (hence, rules or lexicon are not the main requirement to process data). In a nutshell, I would refine the argument or nuance it.

*Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5, 135-146.
**Akbik, A., Blythe, D., & Vollgraf, R. (2018, August). Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on Computational Linguistics, 1638-1649

> Hoang: distributed word representations (glove, word2vec) constructed from context can capture semantics based on the hypothesis that synonyms often share similar contextual words. So synonyms like "headache" and "cephalea" can have close representations if they share many contextual words like "head" or "pain". I will elaborate more on this argument favoring neural approaches  in "Hoang - JBMS-minor R3P2".

> Min: better, but doesn't really address problems with syntax and word movement.  You can describe how embeddings like BERT address word ordering to a certain extent and then pass to a suitable analysis of BERT reference.

-Section 5, Discussion, p. 12, l. 34ff: Authors pinpoint that the problems when using the UMLS are not strictly related to the lexicon-based approach, but rather to the processing of terms or the matching strategy without considering the context. This can be alleviated with post-processing rules.

> Hoang: post-processing rules are possible but not scalable to engineer. Unsupervised approaches to context encoding for UMLS or general feature-based approaches might be efficient but challenging, which is left open for future works.

> Min: state the planned action for this (here and for other feedback items).  It is important the reviewer know what you did in reaction to their comments.  

-P. 12, l. 54, "Limitations": I agree with the authors that "user's credibility" can be harmed when users do not give enough "thanks" to posts which convey reliable information; and vice versa, users with a high number of thanks might state wrong information, which could receive many thanks because it is well presented and widely trusted...
I think another limitation should be commented on regarding the reliability of the experimental data. The "ground truth" in these posts only relies on the annotation of terms expressing drug side effects from the Mayo Clinic database. To generalize the experiments to a broader community (e.g. Medical informatics), the annotation of these posts should be checked by healthcare professionals---e.g. pharmacovigilance experts. At least a small percent, or just focusing on a subset of drugs. I think authors should add this limitation and consider it as a future task to extend and improve their work.

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R3P4"

> Min: say more in your own words and read https://en.wikipedia.org/wiki/Pharmacovigilance and see whether any of the general references there are good citations to add after skimming them.

Lastly, there are still some spelling errors to be corrected:

-Throughout the article (and including the abstract: e.g. "her credibility"), the pronoun "her" or "she" is used to refer to "user". I recommend authors change it to a neuter form (e.g. "their") or use "his/her", "he/she", as in p. 5.
-Abstract, "Conclusions": I would reorder "feasible": "...making feasible a self-supervised approach..."
-P. 2: "involved drugs..." -> "involving drugs..."
-P. 3: Please, reword the following sentence, the sense is unclear: "We review the related work as per our listed contribution." L. 17: garnerded -> "gathered"? "achieved"?
-P. 4: In the following sentence, "it" can be ambiguous and can refer to "our work"; please, reword: "As our work makes use of the rich topographical properties of online communities, it also helps to briefly review..."
-P. 4: "We first define some terminologies" -> "...define some terms and formalize them as follows"?
-P. 5, l. 47: "fuly supervised learning" -> "fully"
-P. 5: "via a drug-side effect database": Authors mention later in the article (in page 9, Section 4) that they used the Mayo Clinic medical database; I think this could also be commented here.
-P. 6, l. 30: "is of 2-fold" -> "is 2-fold"
-P. 6, l. 42: "The top 5 most common side effects in each clusters are demonstrated in Table 3" -> "are shown in Table 3"
-P. 7: There is a missing closing bracket after "Figure 1".
-P. 10: Please cite the UMLS as follows: Bodenreider, Olivier. The unified medical language system (UMLS): integrating biomedical terminology. Nucleic acids research, 2004, vol. 32, no suppl_1, p. D267-D270.
-P. 12, l. 15: Something is missing in the following sentence: "while optimizing the * without being aware of users. "
-Table 10: Regarding the examples obtained by NEAT, where they obtained with the LSTM architecture, by the CNN model, or both (if so, this could be added as a positive outcome of their approach)?

> Hoang: agree, revised as suggested in "Hoang - JBMS-minor R3P5"

===============================================================================
===============================================================================
===============================================================================
Major revision (29/10/2019):

Reviewer 1:

1. In page 2 author mention "trustworthy user-generated content", what does this mean? what is trustworthy? Does it have labels or scales on how to ensure trustworthiness?

> Hoang: 
- Credibility modeling is an important aspect in large scale knowledge harvesting system.
- We model trustworthiness of information in the discussion representation learning that follow general principle of truth discovery in previous work.
- The dataset provides no explicit ground truths for credibility, hence we use a credibility proxy of number of ``thanks'' received by other users
 
2. In page 2  author mention "quality of user-generated content", what is the meaning of quality in the scenario of user-generated content. In the online world of social media, it is believed to be noisy, in such setting who can check this quality, particularly in a health-based discussion forum?

> Hoang: Unlike previous work, we are not very concerned about measuring credibility of individual statement. We are more concerned about user credibility in order to encode our thread representation more effectively.

3. "We build neural architecture"- Why we need this? why not other simpler models? What is the motivation of this? The justification is not convincing. The author jumps to the solution approach without clearly describing the problem.

> Hoang: The main supports for neural networks are
- Modeling lexical variations in user-generated content
- Modeling long-term dependencies in long structured document, especially in community question answering with hierarchical networks.
- Modular design that can easily integrate new components or update existing ones such as text encoders.

4. In the task definition, what does the binary side effect label vector means? I could not get this?

> Hoang: truth vector of size = number of side effect where index i = 1 if side effect i is present, 0 otherwise.

5. In a User Clustering section author mention "Set of meaning cluster C", how can one ensure the meaningful cluster? how does the author know the clusters are meaningful? The author should provide statistical evidence of meaningful clusters.

> Hoang: Provided the statistical significance in term of silhouette score, also provided top common side effects from each cluster for observation

6. The author proposed a model which has a very marginal improvement over the state of the art methods. Are these improvements really significant?

> Hoang: conducted 1-tail t-test at p < 0.05.

7. In a situation like side effects prediction finding correctly predicting positive label is important, which means the metric like AUC-PR would have been very useful. From precision, perspective WPE has outperformed all the other algorithms. This means (WPE+ Credibility weights) demonstrated better performance for correctly classifying side effect labels, then why we need your proposed model which takes 3 components (CW+UE+CA) and still perform less accurate in terms of precision.

> Hoang: in the task of drug side effect discovery, we would like to balance between confirming existing side effects, measured in term of precision, and covering new side effects, measured in term of recall. Therefore, we quantify our improvement mainly in term of F1 score, which is the harmonic mean of precision and recall. In this aspect, our proposed model achieves incremental improvement via ablation study.

8. The author should report the performance of algorithms by taking CW, UE, and CA individually. This is because it is not clear the contribution of these components for the performance in accuracy.

> Hoang: reported in the appendix

9. Can (CW + UE + CA) all three components be incorporated in WPE ? If so then the performance of WPE should also be demonstrated?

> Hoang: I think the author misunderstood the results. We report the result of all three components under NEAT system.

10. Since this is a multilabel prediction problem the author should compare (i) random baseline (ii) the baseline supervised model such as simple SVM or any model of author's choice taking word features as a "bag-words" and report the performance as a simple baseline.

> Hoang: I find the random baseline trivial. Instead of implementing random baseline I randomized the user identity of each post in order to demonstrate that our system has learned to incorporated user-specific information into its decision.

11. The author has not done the significance test for the result reported in Table 4. Are these 10 fold cross validation scores from the authors model significantly different than the baseline methods? Simple t-test reporting p-values should help.

> Hoang: same concerned with comment 6

12. In a "Microscopic Analysis" how does the author gather these sample in Table 6?  Are these sample randomly picked from a test set? What is the significance of the per sample prediction based on credibility score?

> Hoang: I change the microscopic analysis into 2 extra experiment settings:
- User credibility analysis: I measure Spearman's coefficient and nDCG@2 in ranking the trustworthy users based on their received number of 'thanks'.
- Attention-based Side Effect Extraction: I report statistical significance of benchmark against a medical term tagging baseline and a state-of-the-art neural extractor in retrieving mentioned ADR. The example in table 10 was chosen at the first example in the test subset where NEAT Attention performs better than both baselines.

13. If the side-effects labels are randomized what is the probability of predicting the same prediction close to Ground Truth using author's proposed model please report the statistical significance of the per sample prediction in Table 6.

> Hoang: Statistical significance is reported by experiments in table 9, random baseline is reported in table 7 and 8

14. The author should provide the xlsx sheet or the text file for the ranked side effects prediction as supplementary material for the test sets.

> Hoang: included 3 models output extracted side effects in the revised submission

15. The meaning of red dots and blue dots in Figure 5 should be described in the caption.

> Hoang: no longer valid as the figures are dismissed

16. From Figure 5 scatter plot that there is no clear linear association in the accuracy measure with Avg Thread score and performance metrics. Why the author is using linear correlation to interpret this? I could not quite get this. What will be the justification if Spearman correlation is used in this from the perspective or ranked results?

> Hoang: I reported the measurement of Spearman and nDCG@2 in table 6

17. There is a lot of similar work in the area of side-effect drugs prediction. The author has not covered most of the literature.  As the author is focusing on the computational part they should cover the semi-supervised and unsupervised machine learning areas of side effect drug prediction and should critically evaluate with their proposed model.

> Hoang: I covered more literatures, included the benchmark with state-of-the-art ADR extraction.

18. The author pointed the datasets but they should make their code public either in GitHub or other repositories to reproduce their results.

> Hoang: I release the implementation

19. This work has been published in the ACL anthology Workshop in Text Mining (https://www.aclweb.org/anthology/papers/W/W18/W18-5602/).  The author should clearly make a difference with the journal version. With this version, I am not seeing any major changes except the Pearson correlation and little justification. Even the tables are the same. If this is an invited journal the author should cite the workshop paper. There are lots of copying and pasting which would raise an issue of plagiarism.

> Hoang: I cited the workshop paper I highlighted the different in this journal submission towards the end of Introduction session.

----------
Reviewer 2:

1. It is also not possible to attest for the novelty of this approach as the related work does not provide sufficient information.

> Hoang: I updated the literature review

2. The authors base the reasoning to choose a neural network on only two papers. No references are given for the application of an attention mechanism in other works regardless of the application field.

> Hoang: I include justification for neural method under section 2.3 and the cover letter. I justified the implementation of attention mechanism in section 4: User cluster attention and the cover letter

3. On page 8 "Thread Content Encoding with Credibility Weights", there is no clear reasoning for the credibility component. A reference is also lacking for "truth discovery mechanism". What describes "credibility" here? Is a credible user simply an experienced user? According to the authors reasoning, experienced users (i.e. users with a lot of posts/responses) are the major reason for the credibility of the thread content.

> Hoang: similar concern with Reviewer 1's Comment 1, as addressed.

4. On the Experiments section, the authors state the source of the drug side-effects but not of the drug names; where does this list come from? They also make no reference to posts where a user might combine two or more drugs; how are the side-effects predicted for these cases? Or are these posts removed from the dataset?

> Hoang: the list of side effects come from a drug-side effects mapping from Mayo clinic portal. Each discussion is labeled with a list of mentioned drugs which are used to derived the side effects.

5. On page 10, the WPE baseline is stated as not containing the user expertise (UE), however, according to page 8 the component credibility weight (CW) requires the UE.

> Hoang: Credibility Weight does not consider user expertise. It solely concern with the user scores assigned and optimized by our system.

6. On the Experimental Settings subsection, it is not detailed if the parameters applied in all the stated models were randomly chosen, selected according to related work, or tested and shown to be the most adequate for this work. The reason for the implementation of experiment 2 is not stated here nor put in line with the goal of the paper. How does this contribute to the work? In the following section, the authors state that "the attention mechanism is more effective when the drugs are present"; this could have been said without experiment 2. As the goal in the work is to predict drug side-effects and given that the drug names is not a proposed restriction, I see no reason for this experiment.

> Hoang: I agree with this and dismissed experiment 2. I also stated the parameter choice in experiment settings.

7. On the Results and Discussion section, the less pronounced improvements of incorporating UE can be attributed to its indirect use in the WPE system.

> Hoang: The updated results are no longer less pronounced.

8. On page 13 and 14, the Pearson values are meaningless since no significance test result is present. The authors state that "the learned user scores correlate with their interaction level", since the user score is calculated by also depending on the user experience and, hence, the participation in the forum, this is an obvious statement.

> Hoang: No longer valid with updated credibility definition and analysis

----------
Reviewer 3

1. Still, I did not agree with the authors' manner to compute the "credibility scores". An "active and enthusiastic contributor" (in authors' words) does not necessarily imply that he/she may express a correct, reasonable or sensible content. An example may be any fanatic user from the anti-vaccines movement, who might be very participative and present in the social media to oppose childhood vaccination, but he/she might defend views that are not supported by the medical scientific community.

> Hoang: No longer valid with updated credibility definition and analysis

2. I also missed some details on how authors map the terms extracted from the Mayo Clinic and the terms or mentions of side effects in the web posts. Many terms in user posts will certainly have mispellings, appear in abbreviations, etc; these are recurrent challenges when working with user generated content.

> Hoang: Released the source code. Basically I do lemmatization and extract the exact match. If it's an n-gram (n>1) side effect I impose a window of size 5 for co-occurrence of single token of the n-grams.

----------------------------------------

